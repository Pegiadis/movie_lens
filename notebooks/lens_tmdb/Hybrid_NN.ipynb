{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T15:42:39.086104261Z",
     "start_time": "2023-08-03T15:42:38.387403607Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "movies = pd.read_csv('../../data/lens_tmdb/cleaned/df_all.csv')\n",
    "ratings = pd.read_csv('../../data/lens_tmdb/ratings_small.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T15:43:47.990266304Z",
     "start_time": "2023-08-03T15:43:47.419867742Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "movies = movies[['movieId', 'genre', 'director']]  # select features to use\n",
    "data = pd.merge(ratings, movies, on='movieId')\n",
    "data = data[['userId', 'movieId', 'rating', 'genre', 'director']]\n",
    "\n",
    "# Label encoding\n",
    "for col in ['userId', 'movieId', 'genre', 'director']:\n",
    "    data[col] = LabelEncoder().fit_transform(data[col])\n",
    "\n",
    "# Train test split\n",
    "train, test = train_test_split(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T15:43:48.151230777Z",
     "start_time": "2023-08-03T15:43:48.101340893Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Define model\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, n_genres, n_directors, emb_size):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, emb_size)\n",
    "        self.movie_embedding = nn.Embedding(n_movies, emb_size)\n",
    "        self.genre_embedding = nn.Embedding(n_genres, emb_size)\n",
    "        self.director_embedding = nn.Embedding(n_directors, emb_size)\n",
    "        self.fc = nn.Linear(emb_size*4, 1)\n",
    "\n",
    "    def forward(self, user, movie, genre, director):\n",
    "        user_emb = self.user_embedding(user)\n",
    "        movie_emb = self.movie_embedding(movie)\n",
    "        genre_emb = self.genre_embedding(genre)\n",
    "        director_emb = self.director_embedding(director)\n",
    "        x = torch.cat([user_emb, movie_emb, genre_emb, director_emb], dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T15:44:03.041070784Z",
     "start_time": "2023-08-03T15:44:03.036768703Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = HybridModel(\n",
    "    n_users=data['userId'].nunique(),\n",
    "    n_movies=data['movieId'].nunique(),\n",
    "    n_genres=data['genre'].nunique(),\n",
    "    n_directors=data['director'].nunique(),\n",
    "    emb_size=100  # size of the embedding vectors\n",
    ")\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T15:44:15.433011323Z",
     "start_time": "2023-08-03T15:44:15.387194844Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.645596444606781\n",
      "Epoch: 1 Loss: 0.6454698443412781\n",
      "Epoch: 2 Loss: 0.6453450322151184\n",
      "Epoch: 3 Loss: 0.6452221274375916\n",
      "Epoch: 4 Loss: 0.6451009511947632\n",
      "Epoch: 5 Loss: 0.6449815630912781\n",
      "Epoch: 6 Loss: 0.6448639631271362\n",
      "Epoch: 7 Loss: 0.6447480916976929\n",
      "Epoch: 8 Loss: 0.6446338891983032\n",
      "Epoch: 9 Loss: 0.6445212960243225\n",
      "Epoch: 10 Loss: 0.6444104313850403\n",
      "Epoch: 11 Loss: 0.644301176071167\n",
      "Epoch: 12 Loss: 0.6441934704780579\n",
      "Epoch: 13 Loss: 0.6440874338150024\n",
      "Epoch: 14 Loss: 0.6439828276634216\n",
      "Epoch: 15 Loss: 0.6438798308372498\n",
      "Epoch: 16 Loss: 0.643778383731842\n",
      "Epoch: 17 Loss: 0.6436783671379089\n",
      "Epoch: 18 Loss: 0.6435797810554504\n",
      "Epoch: 19 Loss: 0.6434826850891113\n",
      "Epoch: 20 Loss: 0.6433870792388916\n",
      "Epoch: 21 Loss: 0.6432927846908569\n",
      "Epoch: 22 Loss: 0.6431999206542969\n",
      "Epoch: 23 Loss: 0.6431083679199219\n",
      "Epoch: 24 Loss: 0.6430181264877319\n",
      "Epoch: 25 Loss: 0.6429293155670166\n",
      "Epoch: 26 Loss: 0.6428418159484863\n",
      "Epoch: 27 Loss: 0.6427554488182068\n",
      "Epoch: 28 Loss: 0.6426704525947571\n",
      "Epoch: 29 Loss: 0.6425867676734924\n",
      "Epoch: 30 Loss: 0.6425042152404785\n",
      "Epoch: 31 Loss: 0.6424228549003601\n",
      "Epoch: 32 Loss: 0.6423428058624268\n",
      "Epoch: 33 Loss: 0.6422638893127441\n",
      "Epoch: 34 Loss: 0.6421860456466675\n",
      "Epoch: 35 Loss: 0.6421094536781311\n",
      "Epoch: 36 Loss: 0.6420339941978455\n",
      "Epoch: 37 Loss: 0.6419596076011658\n",
      "Epoch: 38 Loss: 0.6418863534927368\n",
      "Epoch: 39 Loss: 0.6418140530586243\n",
      "Epoch: 40 Loss: 0.6417428851127625\n",
      "Epoch: 41 Loss: 0.6416727900505066\n",
      "Epoch: 42 Loss: 0.6416037082672119\n",
      "Epoch: 43 Loss: 0.6415356397628784\n",
      "Epoch: 44 Loss: 0.6414686441421509\n",
      "Epoch: 45 Loss: 0.6414026021957397\n",
      "Epoch: 46 Loss: 0.6413374543190002\n",
      "Epoch: 47 Loss: 0.6412733197212219\n",
      "Epoch: 48 Loss: 0.64121013879776\n",
      "Epoch: 49 Loss: 0.6411478519439697\n",
      "Epoch: 50 Loss: 0.6410865187644958\n",
      "Epoch: 51 Loss: 0.6410260796546936\n",
      "Epoch: 52 Loss: 0.640966534614563\n",
      "Epoch: 53 Loss: 0.6409078240394592\n",
      "Epoch: 54 Loss: 0.6408500075340271\n",
      "Epoch: 55 Loss: 0.6407930254936218\n",
      "Epoch: 56 Loss: 0.6407369375228882\n",
      "Epoch: 57 Loss: 0.6406816840171814\n",
      "Epoch: 58 Loss: 0.6406272053718567\n",
      "Epoch: 59 Loss: 0.6405735015869141\n",
      "Epoch: 60 Loss: 0.6405205726623535\n",
      "Epoch: 61 Loss: 0.6404685378074646\n",
      "Epoch: 62 Loss: 0.640417218208313\n",
      "Epoch: 63 Loss: 0.6403666138648987\n",
      "Epoch: 64 Loss: 0.6403167843818665\n",
      "Epoch: 65 Loss: 0.6402677297592163\n",
      "Epoch: 66 Loss: 0.6402192711830139\n",
      "Epoch: 67 Loss: 0.6401716470718384\n",
      "Epoch: 68 Loss: 0.6401246786117554\n",
      "Epoch: 69 Loss: 0.6400784254074097\n",
      "Epoch: 70 Loss: 0.6400328278541565\n",
      "Epoch: 71 Loss: 0.6399878859519958\n",
      "Epoch: 72 Loss: 0.6399437189102173\n",
      "Epoch: 73 Loss: 0.6399000883102417\n",
      "Epoch: 74 Loss: 0.6398570537567139\n",
      "Epoch: 75 Loss: 0.6398147940635681\n",
      "Epoch: 76 Loss: 0.6397731304168701\n",
      "Epoch: 77 Loss: 0.6397320032119751\n",
      "Epoch: 78 Loss: 0.6396915316581726\n",
      "Epoch: 79 Loss: 0.6396517157554626\n",
      "Epoch: 80 Loss: 0.6396123766899109\n",
      "Epoch: 81 Loss: 0.6395736932754517\n",
      "Epoch: 82 Loss: 0.6395355463027954\n",
      "Epoch: 83 Loss: 0.6394979357719421\n",
      "Epoch: 84 Loss: 0.6394609212875366\n",
      "Epoch: 85 Loss: 0.6394244432449341\n",
      "Epoch: 86 Loss: 0.6393885612487793\n",
      "Epoch: 87 Loss: 0.6393530964851379\n",
      "Epoch: 88 Loss: 0.6393182277679443\n",
      "Epoch: 89 Loss: 0.6392838358879089\n",
      "Epoch: 90 Loss: 0.6392499208450317\n",
      "Epoch: 91 Loss: 0.6392166018486023\n",
      "Epoch: 92 Loss: 0.639183759689331\n",
      "Epoch: 93 Loss: 0.6391513347625732\n",
      "Epoch: 94 Loss: 0.6391194462776184\n",
      "Epoch: 95 Loss: 0.6390880346298218\n",
      "Epoch: 96 Loss: 0.6390569806098938\n",
      "Epoch: 97 Loss: 0.6390265226364136\n",
      "Epoch: 98 Loss: 0.638996422290802\n",
      "Epoch: 99 Loss: 0.6389667987823486\n",
      "Epoch: 100 Loss: 0.6389375925064087\n",
      "Epoch: 101 Loss: 0.638908863067627\n",
      "Epoch: 102 Loss: 0.6388805508613586\n",
      "Epoch: 103 Loss: 0.638852596282959\n",
      "Epoch: 104 Loss: 0.6388251185417175\n",
      "Epoch: 105 Loss: 0.6387979388237\n",
      "Epoch: 106 Loss: 0.6387712955474854\n",
      "Epoch: 107 Loss: 0.6387450098991394\n",
      "Epoch: 108 Loss: 0.6387190818786621\n",
      "Epoch: 109 Loss: 0.6386935114860535\n",
      "Epoch: 110 Loss: 0.6386682987213135\n",
      "Epoch: 111 Loss: 0.6386435627937317\n",
      "Epoch: 112 Loss: 0.6386191248893738\n",
      "Epoch: 113 Loss: 0.6385950446128845\n",
      "Epoch: 114 Loss: 0.6385713219642639\n",
      "Epoch: 115 Loss: 0.6385480165481567\n",
      "Epoch: 116 Loss: 0.6385249495506287\n",
      "Epoch: 117 Loss: 0.638502299785614\n",
      "Epoch: 118 Loss: 0.6384798884391785\n",
      "Epoch: 119 Loss: 0.6384579539299011\n",
      "Epoch: 120 Loss: 0.6384363174438477\n",
      "Epoch: 121 Loss: 0.6384148597717285\n",
      "Epoch: 122 Loss: 0.6383938193321228\n",
      "Epoch: 123 Loss: 0.6383730173110962\n",
      "Epoch: 124 Loss: 0.6383525729179382\n",
      "Epoch: 125 Loss: 0.6383324861526489\n",
      "Epoch: 126 Loss: 0.6383126378059387\n",
      "Epoch: 127 Loss: 0.6382930874824524\n",
      "Epoch: 128 Loss: 0.6382737755775452\n",
      "Epoch: 129 Loss: 0.6382548809051514\n",
      "Epoch: 130 Loss: 0.6382361054420471\n",
      "Epoch: 131 Loss: 0.6382176876068115\n",
      "Epoch: 132 Loss: 0.6381996273994446\n",
      "Epoch: 133 Loss: 0.6381816864013672\n",
      "Epoch: 134 Loss: 0.6381640434265137\n",
      "Epoch: 135 Loss: 0.6381466388702393\n",
      "Epoch: 136 Loss: 0.6381295323371887\n",
      "Epoch: 137 Loss: 0.6381126642227173\n",
      "Epoch: 138 Loss: 0.6380960941314697\n",
      "Epoch: 139 Loss: 0.6380797624588013\n",
      "Epoch: 140 Loss: 0.6380636096000671\n",
      "Epoch: 141 Loss: 0.6380476951599121\n",
      "Epoch: 142 Loss: 0.638032078742981\n",
      "Epoch: 143 Loss: 0.6380166411399841\n",
      "Epoch: 144 Loss: 0.6380014419555664\n",
      "Epoch: 145 Loss: 0.6379864811897278\n",
      "Epoch: 146 Loss: 0.6379716992378235\n",
      "Epoch: 147 Loss: 0.6379572153091431\n",
      "Epoch: 148 Loss: 0.6379428505897522\n",
      "Epoch: 149 Loss: 0.6379287242889404\n",
      "Epoch: 150 Loss: 0.6379148364067078\n",
      "Epoch: 151 Loss: 0.6379011273384094\n",
      "Epoch: 152 Loss: 0.637887716293335\n",
      "Epoch: 153 Loss: 0.6378743648529053\n",
      "Epoch: 154 Loss: 0.6378612518310547\n",
      "Epoch: 155 Loss: 0.6378483772277832\n",
      "Epoch: 156 Loss: 0.6378355622291565\n",
      "Epoch: 157 Loss: 0.6378231048583984\n",
      "Epoch: 158 Loss: 0.6378107666969299\n",
      "Epoch: 159 Loss: 0.6377986073493958\n",
      "Epoch: 160 Loss: 0.6377866268157959\n",
      "Epoch: 161 Loss: 0.6377747654914856\n",
      "Epoch: 162 Loss: 0.6377631425857544\n",
      "Epoch: 163 Loss: 0.6377516388893127\n",
      "Epoch: 164 Loss: 0.6377403736114502\n",
      "Epoch: 165 Loss: 0.6377291679382324\n",
      "Epoch: 166 Loss: 0.6377182006835938\n",
      "Epoch: 167 Loss: 0.6377074718475342\n",
      "Epoch: 168 Loss: 0.6376968026161194\n",
      "Epoch: 169 Loss: 0.6376863121986389\n",
      "Epoch: 170 Loss: 0.6376760005950928\n",
      "Epoch: 171 Loss: 0.6376657485961914\n",
      "Epoch: 172 Loss: 0.6376557350158691\n",
      "Epoch: 173 Loss: 0.6376458406448364\n",
      "Epoch: 174 Loss: 0.637636125087738\n",
      "Epoch: 175 Loss: 0.6376265287399292\n",
      "Epoch: 176 Loss: 0.6376170516014099\n",
      "Epoch: 177 Loss: 0.6376076936721802\n",
      "Epoch: 178 Loss: 0.63759845495224\n",
      "Epoch: 179 Loss: 0.6375895142555237\n",
      "Epoch: 180 Loss: 0.6375805735588074\n",
      "Epoch: 181 Loss: 0.6375717520713806\n",
      "Epoch: 182 Loss: 0.637563169002533\n",
      "Epoch: 183 Loss: 0.6375545859336853\n",
      "Epoch: 184 Loss: 0.637546181678772\n",
      "Epoch: 185 Loss: 0.6375378966331482\n",
      "Epoch: 186 Loss: 0.637529730796814\n",
      "Epoch: 187 Loss: 0.6375216841697693\n",
      "Epoch: 188 Loss: 0.6375137567520142\n",
      "Epoch: 189 Loss: 0.6375059485435486\n",
      "Epoch: 190 Loss: 0.6374981999397278\n",
      "Epoch: 191 Loss: 0.6374906897544861\n",
      "Epoch: 192 Loss: 0.6374831795692444\n",
      "Epoch: 193 Loss: 0.637475848197937\n",
      "Epoch: 194 Loss: 0.6374685764312744\n",
      "Epoch: 195 Loss: 0.6374614238739014\n",
      "Epoch: 196 Loss: 0.6374543905258179\n",
      "Epoch: 197 Loss: 0.6374474763870239\n",
      "Epoch: 198 Loss: 0.6374406218528748\n",
      "Epoch: 199 Loss: 0.6374338865280151\n",
      "Epoch: 200 Loss: 0.6374272704124451\n",
      "Epoch: 201 Loss: 0.6374207735061646\n",
      "Epoch: 202 Loss: 0.6374143362045288\n",
      "Epoch: 203 Loss: 0.6374079585075378\n",
      "Epoch: 204 Loss: 0.6374017000198364\n",
      "Epoch: 205 Loss: 0.6373955607414246\n",
      "Epoch: 206 Loss: 0.6373894214630127\n",
      "Epoch: 207 Loss: 0.6373835206031799\n",
      "Epoch: 208 Loss: 0.6373775601387024\n",
      "Epoch: 209 Loss: 0.637371838092804\n",
      "Epoch: 210 Loss: 0.6373660564422607\n",
      "Epoch: 211 Loss: 0.6373603940010071\n",
      "Epoch: 212 Loss: 0.6373547911643982\n",
      "Epoch: 213 Loss: 0.6373493671417236\n",
      "Epoch: 214 Loss: 0.6373439431190491\n",
      "Epoch: 215 Loss: 0.6373386383056641\n",
      "Epoch: 216 Loss: 0.6373333930969238\n",
      "Epoch: 217 Loss: 0.6373282670974731\n",
      "Epoch: 218 Loss: 0.6373232007026672\n",
      "Epoch: 219 Loss: 0.6373181343078613\n",
      "Epoch: 220 Loss: 0.637313187122345\n",
      "Epoch: 221 Loss: 0.6373082995414734\n",
      "Epoch: 222 Loss: 0.6373035311698914\n",
      "Epoch: 223 Loss: 0.6372988224029541\n",
      "Epoch: 224 Loss: 0.6372941732406616\n",
      "Epoch: 225 Loss: 0.6372895836830139\n",
      "Epoch: 226 Loss: 0.637285053730011\n",
      "Epoch: 227 Loss: 0.6372805237770081\n",
      "Epoch: 228 Loss: 0.6372761726379395\n",
      "Epoch: 229 Loss: 0.6372718214988708\n",
      "Epoch: 230 Loss: 0.6372675895690918\n",
      "Epoch: 231 Loss: 0.6372634172439575\n",
      "Epoch: 232 Loss: 0.637259304523468\n",
      "Epoch: 233 Loss: 0.6372551321983337\n",
      "Epoch: 234 Loss: 0.6372511982917786\n",
      "Epoch: 235 Loss: 0.6372472047805786\n",
      "Epoch: 236 Loss: 0.6372432708740234\n",
      "Epoch: 237 Loss: 0.6372394561767578\n",
      "Epoch: 238 Loss: 0.637235701084137\n",
      "Epoch: 239 Loss: 0.6372319459915161\n",
      "Epoch: 240 Loss: 0.6372283101081848\n",
      "Epoch: 241 Loss: 0.6372246742248535\n",
      "Epoch: 242 Loss: 0.6372210383415222\n",
      "Epoch: 243 Loss: 0.6372175812721252\n",
      "Epoch: 244 Loss: 0.6372140049934387\n",
      "Epoch: 245 Loss: 0.6372106075286865\n",
      "Epoch: 246 Loss: 0.6372072696685791\n",
      "Epoch: 247 Loss: 0.6372039914131165\n",
      "Epoch: 248 Loss: 0.637200653553009\n",
      "Epoch: 249 Loss: 0.6371974945068359\n",
      "Epoch: 250 Loss: 0.6371942758560181\n",
      "Epoch: 251 Loss: 0.637191116809845\n",
      "Epoch: 252 Loss: 0.6371880769729614\n",
      "Epoch: 253 Loss: 0.6371850371360779\n",
      "Epoch: 254 Loss: 0.6371821165084839\n",
      "Epoch: 255 Loss: 0.6371790766716003\n",
      "Epoch: 256 Loss: 0.6371761560440063\n",
      "Epoch: 257 Loss: 0.6371733546257019\n",
      "Epoch: 258 Loss: 0.6371704936027527\n",
      "Epoch: 259 Loss: 0.637167751789093\n",
      "Epoch: 260 Loss: 0.6371649503707886\n",
      "Epoch: 261 Loss: 0.6371623277664185\n",
      "Epoch: 262 Loss: 0.6371596455574036\n",
      "Epoch: 263 Loss: 0.6371570229530334\n",
      "Epoch: 264 Loss: 0.6371544003486633\n",
      "Epoch: 265 Loss: 0.6371518969535828\n",
      "Epoch: 266 Loss: 0.6371493935585022\n",
      "Epoch: 267 Loss: 0.6371469497680664\n",
      "Epoch: 268 Loss: 0.6371445059776306\n",
      "Epoch: 269 Loss: 0.6371420621871948\n",
      "Epoch: 270 Loss: 0.6371396780014038\n",
      "Epoch: 271 Loss: 0.6371373534202576\n",
      "Epoch: 272 Loss: 0.6371350884437561\n",
      "Epoch: 273 Loss: 0.6371327638626099\n",
      "Epoch: 274 Loss: 0.637130618095398\n",
      "Epoch: 275 Loss: 0.6371284127235413\n",
      "Epoch: 276 Loss: 0.6371262073516846\n",
      "Epoch: 277 Loss: 0.6371241211891174\n",
      "Epoch: 278 Loss: 0.6371220350265503\n",
      "Epoch: 279 Loss: 0.6371199488639832\n",
      "Epoch: 280 Loss: 0.6371179223060608\n",
      "Epoch: 281 Loss: 0.6371158957481384\n",
      "Epoch: 282 Loss: 0.6371138691902161\n",
      "Epoch: 283 Loss: 0.6371119022369385\n",
      "Epoch: 284 Loss: 0.6371100544929504\n",
      "Epoch: 285 Loss: 0.6371081471443176\n",
      "Epoch: 286 Loss: 0.6371062397956848\n",
      "Epoch: 287 Loss: 0.6371043920516968\n",
      "Epoch: 288 Loss: 0.6371026039123535\n",
      "Epoch: 289 Loss: 0.6371008157730103\n",
      "Epoch: 290 Loss: 0.6370990872383118\n",
      "Epoch: 291 Loss: 0.6370972990989685\n",
      "Epoch: 292 Loss: 0.63709557056427\n",
      "Epoch: 293 Loss: 0.6370939016342163\n",
      "Epoch: 294 Loss: 0.6370922327041626\n",
      "Epoch: 295 Loss: 0.6370906233787537\n",
      "Epoch: 296 Loss: 0.6370890140533447\n",
      "Epoch: 297 Loss: 0.6370874047279358\n",
      "Epoch: 298 Loss: 0.6370858550071716\n",
      "Epoch: 299 Loss: 0.6370843052864075\n",
      "Epoch: 300 Loss: 0.6370828151702881\n",
      "Epoch: 301 Loss: 0.6370812654495239\n",
      "Epoch: 302 Loss: 0.6370798349380493\n",
      "Epoch: 303 Loss: 0.6370783448219299\n",
      "Epoch: 304 Loss: 0.6370769143104553\n",
      "Epoch: 305 Loss: 0.6370755434036255\n",
      "Epoch: 306 Loss: 0.6370741128921509\n",
      "Epoch: 307 Loss: 0.637072741985321\n",
      "Epoch: 308 Loss: 0.6370713710784912\n",
      "Epoch: 309 Loss: 0.6370700597763062\n",
      "Epoch: 310 Loss: 0.6370687484741211\n",
      "Epoch: 311 Loss: 0.637067437171936\n",
      "Epoch: 312 Loss: 0.6370661854743958\n",
      "Epoch: 313 Loss: 0.6370649337768555\n",
      "Epoch: 314 Loss: 0.6370636820793152\n",
      "Epoch: 315 Loss: 0.6370624303817749\n",
      "Epoch: 316 Loss: 0.6370612978935242\n",
      "Epoch: 317 Loss: 0.6370601058006287\n",
      "Epoch: 318 Loss: 0.6370589137077332\n",
      "Epoch: 319 Loss: 0.6370577812194824\n",
      "Epoch: 320 Loss: 0.6370566487312317\n",
      "Epoch: 321 Loss: 0.6370554566383362\n",
      "Epoch: 322 Loss: 0.637054443359375\n",
      "Epoch: 323 Loss: 0.637053370475769\n",
      "Epoch: 324 Loss: 0.6370522975921631\n",
      "Epoch: 325 Loss: 0.6370512247085571\n",
      "Epoch: 326 Loss: 0.6370501518249512\n",
      "Epoch: 327 Loss: 0.63704913854599\n",
      "Epoch: 328 Loss: 0.6370481848716736\n",
      "Epoch: 329 Loss: 0.6370471715927124\n",
      "Epoch: 330 Loss: 0.6370461583137512\n",
      "Epoch: 331 Loss: 0.6370452046394348\n",
      "Epoch: 332 Loss: 0.6370442509651184\n",
      "Epoch: 333 Loss: 0.6370433568954468\n",
      "Epoch: 334 Loss: 0.6370424032211304\n",
      "Epoch: 335 Loss: 0.6370415091514587\n",
      "Epoch: 336 Loss: 0.6370406150817871\n",
      "Epoch: 337 Loss: 0.6370397210121155\n",
      "Epoch: 338 Loss: 0.6370388269424438\n",
      "Epoch: 339 Loss: 0.637037992477417\n",
      "Epoch: 340 Loss: 0.6370371580123901\n",
      "Epoch: 341 Loss: 0.6370363235473633\n",
      "Epoch: 342 Loss: 0.6370354890823364\n",
      "Epoch: 343 Loss: 0.6370346546173096\n",
      "Epoch: 344 Loss: 0.6370338797569275\n",
      "Epoch: 345 Loss: 0.6370331645011902\n",
      "Epoch: 346 Loss: 0.6370323896408081\n",
      "Epoch: 347 Loss: 0.637031614780426\n",
      "Epoch: 348 Loss: 0.637030839920044\n",
      "Epoch: 349 Loss: 0.6370300650596619\n",
      "Epoch: 350 Loss: 0.6370293498039246\n",
      "Epoch: 351 Loss: 0.637028694152832\n",
      "Epoch: 352 Loss: 0.6370279788970947\n",
      "Epoch: 353 Loss: 0.6370272040367126\n",
      "Epoch: 354 Loss: 0.6370265483856201\n",
      "Epoch: 355 Loss: 0.6370258927345276\n",
      "Epoch: 356 Loss: 0.6370251774787903\n",
      "Epoch: 357 Loss: 0.6370245814323425\n",
      "Epoch: 358 Loss: 0.63702392578125\n",
      "Epoch: 359 Loss: 0.6370232701301575\n",
      "Epoch: 360 Loss: 0.6370226740837097\n",
      "Epoch: 361 Loss: 0.6370220184326172\n",
      "Epoch: 362 Loss: 0.6370214223861694\n",
      "Epoch: 363 Loss: 0.6370208263397217\n",
      "Epoch: 364 Loss: 0.6370202302932739\n",
      "Epoch: 365 Loss: 0.637019693851471\n",
      "Epoch: 366 Loss: 0.6370190382003784\n",
      "Epoch: 367 Loss: 0.6370185613632202\n",
      "Epoch: 368 Loss: 0.6370179057121277\n",
      "Epoch: 369 Loss: 0.6370173692703247\n",
      "Epoch: 370 Loss: 0.6370168328285217\n",
      "Epoch: 371 Loss: 0.6370162963867188\n",
      "Epoch: 372 Loss: 0.6370157599449158\n",
      "Epoch: 373 Loss: 0.6370152831077576\n",
      "Epoch: 374 Loss: 0.6370148062705994\n",
      "Epoch: 375 Loss: 0.6370142698287964\n",
      "Epoch: 376 Loss: 0.6370137333869934\n",
      "Epoch: 377 Loss: 0.6370132565498352\n",
      "Epoch: 378 Loss: 0.637012779712677\n",
      "Epoch: 379 Loss: 0.6370123028755188\n",
      "Epoch: 380 Loss: 0.6370118260383606\n",
      "Epoch: 381 Loss: 0.6370113492012024\n",
      "Epoch: 382 Loss: 0.637010931968689\n",
      "Epoch: 383 Loss: 0.6370104551315308\n",
      "Epoch: 384 Loss: 0.6370100378990173\n",
      "Epoch: 385 Loss: 0.6370096206665039\n",
      "Epoch: 386 Loss: 0.6370092034339905\n",
      "Epoch: 387 Loss: 0.637008786201477\n",
      "Epoch: 388 Loss: 0.6370083093643188\n",
      "Epoch: 389 Loss: 0.6370079517364502\n",
      "Epoch: 390 Loss: 0.6370075345039368\n",
      "Epoch: 391 Loss: 0.6370071172714233\n",
      "Epoch: 392 Loss: 0.6370067596435547\n",
      "Epoch: 393 Loss: 0.6370063424110413\n",
      "Epoch: 394 Loss: 0.6370059251785278\n",
      "Epoch: 395 Loss: 0.6370055675506592\n",
      "Epoch: 396 Loss: 0.6370052695274353\n",
      "Epoch: 397 Loss: 0.6370048522949219\n",
      "Epoch: 398 Loss: 0.6370044946670532\n",
      "Epoch: 399 Loss: 0.6370041370391846\n",
      "Epoch: 400 Loss: 0.6370037794113159\n",
      "Epoch: 401 Loss: 0.637003481388092\n",
      "Epoch: 402 Loss: 0.6370031237602234\n",
      "Epoch: 403 Loss: 0.6370028257369995\n",
      "Epoch: 404 Loss: 0.6370024085044861\n",
      "Epoch: 405 Loss: 0.6370021104812622\n",
      "Epoch: 406 Loss: 0.6370018124580383\n",
      "Epoch: 407 Loss: 0.6370015144348145\n",
      "Epoch: 408 Loss: 0.6370011568069458\n",
      "Epoch: 409 Loss: 0.6370008587837219\n",
      "Epoch: 410 Loss: 0.6370006203651428\n",
      "Epoch: 411 Loss: 0.6370002627372742\n",
      "Epoch: 412 Loss: 0.6369999647140503\n",
      "Epoch: 413 Loss: 0.6369996666908264\n",
      "Epoch: 414 Loss: 0.6369994282722473\n",
      "Epoch: 415 Loss: 0.6369991302490234\n",
      "Epoch: 416 Loss: 0.6369988918304443\n",
      "Epoch: 417 Loss: 0.6369985938072205\n",
      "Epoch: 418 Loss: 0.6369982957839966\n",
      "Epoch: 419 Loss: 0.6369980573654175\n",
      "Epoch: 420 Loss: 0.6369977593421936\n",
      "Epoch: 421 Loss: 0.6369975209236145\n",
      "Epoch: 422 Loss: 0.6369972825050354\n",
      "Epoch: 423 Loss: 0.6369970440864563\n",
      "Epoch: 424 Loss: 0.6369968056678772\n",
      "Epoch: 425 Loss: 0.6369965076446533\n",
      "Epoch: 426 Loss: 0.6369962692260742\n",
      "Epoch: 427 Loss: 0.6369960904121399\n",
      "Epoch: 428 Loss: 0.6369958519935608\n",
      "Epoch: 429 Loss: 0.6369956731796265\n",
      "Epoch: 430 Loss: 0.6369954347610474\n",
      "Epoch: 431 Loss: 0.6369951963424683\n",
      "Epoch: 432 Loss: 0.6369948983192444\n",
      "Epoch: 433 Loss: 0.6369947195053101\n",
      "Epoch: 434 Loss: 0.636994481086731\n",
      "Epoch: 435 Loss: 0.6369943618774414\n",
      "Epoch: 436 Loss: 0.6369940638542175\n",
      "Epoch: 437 Loss: 0.636993944644928\n",
      "Epoch: 438 Loss: 0.6369937062263489\n",
      "Epoch: 439 Loss: 0.6369935274124146\n",
      "Epoch: 440 Loss: 0.6369933485984802\n",
      "Epoch: 441 Loss: 0.6369931697845459\n",
      "Epoch: 442 Loss: 0.6369929313659668\n",
      "Epoch: 443 Loss: 0.6369927525520325\n",
      "Epoch: 444 Loss: 0.6369925737380981\n",
      "Epoch: 445 Loss: 0.6369924545288086\n",
      "Epoch: 446 Loss: 0.6369922161102295\n",
      "Epoch: 447 Loss: 0.6369920372962952\n",
      "Epoch: 448 Loss: 0.6369919180870056\n",
      "Epoch: 449 Loss: 0.6369916796684265\n",
      "Epoch: 450 Loss: 0.636991560459137\n",
      "Epoch: 451 Loss: 0.6369913816452026\n",
      "Epoch: 452 Loss: 0.6369912028312683\n",
      "Epoch: 453 Loss: 0.6369910836219788\n",
      "Epoch: 454 Loss: 0.6369908452033997\n",
      "Epoch: 455 Loss: 0.6369907855987549\n",
      "Epoch: 456 Loss: 0.6369906067848206\n",
      "Epoch: 457 Loss: 0.6369904279708862\n",
      "Epoch: 458 Loss: 0.6369903087615967\n",
      "Epoch: 459 Loss: 0.6369901299476624\n",
      "Epoch: 460 Loss: 0.6369900107383728\n",
      "Epoch: 461 Loss: 0.6369898915290833\n",
      "Epoch: 462 Loss: 0.6369897127151489\n",
      "Epoch: 463 Loss: 0.6369895339012146\n",
      "Epoch: 464 Loss: 0.6369894742965698\n",
      "Epoch: 465 Loss: 0.6369892954826355\n",
      "Epoch: 466 Loss: 0.636989176273346\n",
      "Epoch: 467 Loss: 0.6369891166687012\n",
      "Epoch: 468 Loss: 0.6369889378547668\n",
      "Epoch: 469 Loss: 0.6369887590408325\n",
      "Epoch: 470 Loss: 0.6369886994361877\n",
      "Epoch: 471 Loss: 0.6369885206222534\n",
      "Epoch: 472 Loss: 0.6369884014129639\n",
      "Epoch: 473 Loss: 0.6369882822036743\n",
      "Epoch: 474 Loss: 0.6369881629943848\n",
      "Epoch: 475 Loss: 0.63698810338974\n",
      "Epoch: 476 Loss: 0.6369879841804504\n",
      "Epoch: 477 Loss: 0.6369878649711609\n",
      "Epoch: 478 Loss: 0.6369877457618713\n",
      "Epoch: 479 Loss: 0.6369876265525818\n",
      "Epoch: 480 Loss: 0.636987566947937\n",
      "Epoch: 481 Loss: 0.6369874477386475\n",
      "Epoch: 482 Loss: 0.6369873881340027\n",
      "Epoch: 483 Loss: 0.6369871497154236\n",
      "Epoch: 484 Loss: 0.6369871497154236\n",
      "Epoch: 485 Loss: 0.636987030506134\n",
      "Epoch: 486 Loss: 0.6369869112968445\n",
      "Epoch: 487 Loss: 0.6369867920875549\n",
      "Epoch: 488 Loss: 0.6369867324829102\n",
      "Epoch: 489 Loss: 0.6369866132736206\n",
      "Epoch: 490 Loss: 0.6369866132736206\n",
      "Epoch: 491 Loss: 0.6369864344596863\n",
      "Epoch: 492 Loss: 0.6369863748550415\n",
      "Epoch: 493 Loss: 0.6369863152503967\n",
      "Epoch: 494 Loss: 0.6369861960411072\n",
      "Epoch: 495 Loss: 0.6369860768318176\n",
      "Epoch: 496 Loss: 0.6369859576225281\n",
      "Epoch: 497 Loss: 0.6369859576225281\n",
      "Epoch: 498 Loss: 0.6369858384132385\n",
      "Epoch: 499 Loss: 0.6369857788085938\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "def to_tensor(df):\n",
    "    user = torch.tensor(df['userId'].values)\n",
    "    movie = torch.tensor(df['movieId'].values)\n",
    "    genre = torch.tensor(df['genre'].values)\n",
    "    director = torch.tensor(df['director'].values)\n",
    "    rating = torch.tensor(df['rating'].values)\n",
    "    return user, movie, genre, director, rating\n",
    "\n",
    "train_data = to_tensor(train)\n",
    "test_data = to_tensor(test)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):  # number of epochs\n",
    "    user, movie, genre, director, rating = train_data\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(user, movie, genre, director).squeeze()\n",
    "    loss = criterion(outputs, rating.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Epoch:', epoch, 'Loss:', loss.item())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T15:46:27.175742098Z",
     "start_time": "2023-08-03T15:45:42.070485523Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8459954857826233\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    user, movie, genre, director, rating = test_data\n",
    "    outputs = model(user, movie, genre, director).squeeze()\n",
    "    loss = criterion(outputs, rating.float())\n",
    "    print('Test loss:', loss.item())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T15:46:53.997118466Z",
     "start_time": "2023-08-03T15:46:53.958410691Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
